---
title: 'Regression Models: Assignment 1'
author: "Daniel Alonso"
date: "November 24th, 2020"
output: 'pdf_document'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Importing libraries

```{r, echo=TRUE, warning=FALSE, message=FALSE, results="hide"}
library(dplyr)
library(MuMIn)
library(MASS)
```

# Exercise 1

## Simulation

```{r, echo=TRUE, warning=FALSE, results='hide'}
sim = list()
for (j in 1:1000) {
    vals = c()
    for (i in 1:100) {
        run = 3 + 3*cos(i/10 + 50) + rnorm(1, mean=0, sd=1)
        vals = c(vals, run)
    }
    sim[[j]] = vals
}
sim
```

```{r, echo=TRUE, warning=FALSE}
```

# Exercise 2

## Importing the data

```{r, echo=TRUE, warning=FALSE}
d <- data.frame(read.table('../data/index.txt', header=TRUE))
```

```{r, echo=TRUE, warning=FALSE}
X = d$PovPct
Y = d$Brth15to17
beta1 = cov(X, Y)/var(X)
beta0 = mean(Y) - beta1*mean(X)
```

```{r, echo=TRUE, warning=FALSE}
beta1
beta0
```

\newpage

# Exercise 3

First we have the log-likelihood function for $\beta$ and $\sigma^{2}$

$l(\sigma^{2} | X) = \sum_{i=1}^n log(\frac{1}{\sqrt{2 \pi \sigma^{2}}} - \frac{(Y_{i} - (\beta_{0} + \beta_{1} x_{ik} + \dots + \beta_{k} x_{ik}))^{2}}{2 \sigma^{2}})$

$\propto - \frac{n}{2} log(\sigma^{2}) - \frac{(Y - X \beta) \prime (Y - X \beta)}{2 \sigma^{2}}$

Differentiating the second expression:

$\frac{\partial l}{\partial \sigma} ( - \frac{n}{2} log(\sigma^{2}) - \frac{(Y - X \beta) \prime (Y - X \beta)}{2 \sigma^{2}}) = 0$

We get:

$- \frac{n}{2} (\frac{1}{ \sigma^{2}} ) (2 \sigma) - (Y - X \beta) \prime (Y - X \beta) * (-2)(2 \sigma^{-3}) = 0$

We reduce the expression further:

$- \frac{n}{\sigma} +  \frac{(Y - X \beta) \prime (Y - X \beta)}{\sigma^{3}} = 0$

We multiply both sides by $\sigma^{3}$ and we get:

$- n \sigma^{2} +  (Y - X \beta) \prime (Y - X \beta) = 0$

And solving for $\sigma^{2}$ we get:

$\hat{\sigma^{2}} = \frac{(Y - X \beta) \prime (Y - X \beta)}{n}$

Which is our maximum likelihood estimator for $\sigma^2$

# Exercise 4

\newpage

# Exercise 5

```{r, echo=TRUE, warning=FALSE}
bodyfat <- data.frame(read.table('../data/bodyfat.txt', header=TRUE))
modall <- lm(hwfat ~., data = bodyfat)
summary(modall)
```

The sum of residuals is zero:

```{r, echo=TRUE, warning=FALSE}
residuals <- sum(resid(modall))
```

The sum of the observed data is equal to the sum of the fitted values

```{r, echo=TRUE, warning=FALSE}
Y_hat <- predict(modall, bodyfat[1:length(names(bodyfat))-1])
sum(bodyfat$hwfat) - sum(Y_hat)
```

The residuals are orthogonal to the predictors

```{r, echo=TRUE, warning=FALSE}
sum(residuals*bodyfat[1:length(names(bodyfat))-1])
```

The residuals are orthogonal to the fitted values

```{r, echo=TRUE, warning=FALSE}
sum(residuals*Y_hat)
```

\newpage

# Exercise 6

```{r, echo=TRUE, warning=FALSE}
options(na.action = "na.fail")
modall <- lm(hwfat ~., data = bodyfat)
combs <- dredge(modall, extra = "R^2")
print("best model")
combs[combs$"R^2" == max(combs$"R^2")]
```

# Exercise 7

\newpage

# Exercise 8

We define a list with all the models excluding, in each one, a single variable.

```{r, echo=TRUE, warning=FALSE}
models <- list()
vars <- c("age","ht","wt","abs","triceps","subscap")
models[[1]] <- update(modall,.~.-age)
models[[2]] <- update(modall,.~.-ht)
models[[3]] <- update(modall,.~.-wt)
models[[4]] <- update(modall,.~.-abs)
models[[5]] <- update(modall,.~.-triceps)
models[[6]] <- update(modall,.~.-subscap)
```

We run ANOVA with both the models without each variable and the main model including all the other variables.

We can see the pvalues for the ANOVA where each specific variable was excluded:

```{r, echo=TRUE, warning=FALSE}
anovas <- list()
pvalues <- c()
amount_of_vars <- length(names(bodyfat))-1
for (i in 1:amount_of_vars) {
    anovas[[i]] <- anova(models[[i]],modall)
    pvalues <- c(pvalues, sum(anovas[[i]][2,"Pr(>F)"]))
}
for (i in 1:length(vars)) {
    print(paste("excluding: ", vars[i], ": ", pvalues[i] , sep=""))
}
```

Then we compare with summary:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
summary(modall)[4]
```

And we can see we get the same pvalues in the summary. Therefore viewing the summary can be a much faster version of performing such testing.

as a result we get that the least meaningful variable (the variable that explains the lowest variance of the model) is the variable ht (height) followed by the variable wt (weight).

\newpage

# Exercise 9

Given that $E[\hat{Y} | X_{h}] = \hat{Y_{h}} \sim N(X_{h} \beta, \sigma^{2} X_{h} (X^{\prime} X) X_{h}^{\prime})$

$\Rightarrow \hat{y_{h}} \pm t_{n - (k+1), \frac{\alpha}{2}} * \hat{\sigma} \sqrt{h_{ii}}$

where $h_{ii}$ is the diagonal of our $H$ matrix.

is our expression for the $(1-\alpha)\%$ confidence interval for $\hat{Y_{h}}$ when $\sigma^{2}$ is unknown.

# Exercise 10

```{r, echo=TRUE, warning=FALSE, message=FALSE}
transform <- data.frame(read.table('../data/Transform2_V2.txt', header=TRUE))
trm1 <- lm(y2 ~ x1, data=transform)
trm2 <- lm(y2 ~ x2, data=transform)

```