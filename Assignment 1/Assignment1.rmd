---
title: 'Regression Models: Assignment 1'
author: "Daniel Alonso"
date: "November 24th, 2020"
output: 'pdf_document'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = '#>',
fig.path = './figures/'
)
```

## Installing libraries used

```{r, echo=TRUE, warning=FALSE, message=FALSE, results="hide"}
packages = c("dplyr","MuMIn","MASS","leaps","glmnet","car")
for (package in packages) {
    install.packages(package)
}
```

## Importing libraries

```{r, echo=TRUE, warning=FALSE, message=FALSE, results="hide"}
library(dplyr)
library(MuMIn)
library(MASS)
library(leaps)
library(glmnet)
library(car)
```



# Exercise 1

## Simulation

```{r, echo=TRUE, warning=FALSE, results='hide'}
# terms
s = list()
X = list()
# betas estimation
beta_est = list()
beta1hat = c()
beta2hat = c()
# variables
phi = 50
beta1 = 3
beta2 = 3
x1 = matrix(rep(1,100),nrow=100,byrow=T)

# simulation
simulation <- function(mean,sd,phi,beta1,beta2,iters,size) {
    for (j in 1:iters) {
        loop_s = matrix(rep(0,size),nrow=size,byrow=T)
        loop_x2 = matrix(rep(0,size),nrow=size,byrow=T)
        for (i in 1:size) {
            e = rnorm(1, mean=mean, sd=sd)
            loop_x2[i] = cos(i/10 + phi)
            loop_s[i] = beta1 + beta2*loop_x2[i] + e
        }
        X[[j]] = cbind(x1,loop_x2)
        s[[j]] = loop_s
        beta_est[[j]] = ginv(t(X[[j]])%*%X[[j]])%*%t(X[[j]])%*%loop_s
        beta1hat = c(beta1hat, beta_est[[j]][1])
        beta2hat = c(beta2hat, beta_est[[j]][2])
    }
    # plotting beta1hat and beta2hat
    par(mfrow=c(2,1))
    hist(beta1hat)
    hist(beta2hat)
}
```

```{r, echo=TRUE, warning=FALSE, results='hide', fig.height=6}
simulation(mean=0, sd=1, phi=phi, beta1=beta1, beta2=beta2, iters=1000, size=100)
```

We know that the estimate is unbiased if its expected value of our estimators is equal to the real value of our estimators (in this case $E[ \hat{\beta_{1}}]) = \beta_{1}$ and $E[ \hat{\beta_{2}}]) = \beta_{2}$ as the sample size $n \rightarrow \inf$.





# Exercise 2

## Importing the data

```{r, echo=TRUE, warning=FALSE}
d <- data.frame(read.table('../data/index.txt', header=TRUE))
```

```{r, echo=TRUE, warning=FALSE}
X = d$PovPct
Y = d$Brth15to17
beta1 = cov(X, Y)/var(X)
beta0 = mean(Y) - beta1*mean(X)
```

```{r, echo=TRUE, warning=FALSE}
beta1
beta0
```

\newpage

# Exercise 3

First we have the log-likelihood function for $\beta$ and $\sigma^{2}$

$l(\sigma^{2} | X) = \sum_{i=1}^n log(\frac{1}{\sqrt{2 \pi \sigma^{2}}} - \frac{(Y_{i} - (\beta_{0} + \beta_{1} x_{ik} + \dots + \beta_{k} x_{ik}))^{2}}{2 \sigma^{2}})$

$\propto - \frac{n}{2} log(\sigma^{2}) - \frac{(Y - X \beta) \prime (Y - X \beta)}{2 \sigma^{2}}$

Differentiating the second expression:

$\frac{\partial l}{\partial \sigma} ( - \frac{n}{2} log(\sigma^{2}) - \frac{(Y - X \beta) \prime (Y - X \beta)}{2 \sigma^{2}}) = 0$

We get:

$- \frac{n}{2} (\frac{1}{ \sigma^{2}} ) (2 \sigma) - (Y - X \beta) \prime (Y - X \beta) * (-2)(2 \sigma^{-3}) = 0$

We reduce the expression further:

$- \frac{n}{\sigma} +  \frac{(Y - X \beta) \prime (Y - X \beta)}{\sigma^{3}} = 0$

We multiply both sides by $\sigma^{3}$ and we get:

$- n \sigma^{2} +  (Y - X \beta) \prime (Y - X \beta) = 0$

And solving for $\sigma^{2}$ we get:

$\hat{\sigma^{2}} = \frac{(Y - X \beta) \prime (Y - X \beta)}{n}$

Which is our maximum likelihood estimator for $\sigma^2$

# Exercise 4

\newpage

# Exercise 5

```{r, echo=TRUE, warning=FALSE}
bodyfat <- data.frame(read.table('../data/bodyfat.txt', header=TRUE))
modall <- lm(hwfat ~., data = bodyfat)
summary(modall)
```

The sum of residuals is zero:

```{r, echo=TRUE, warning=FALSE}
residuals <- sum(resid(modall))
```

The sum of the observed data is equal to the sum of the fitted values

```{r, echo=TRUE, warning=FALSE}
Y_hat <- predict(modall, bodyfat[1:length(names(bodyfat))-1])
sum(bodyfat$hwfat) - sum(Y_hat)
```

The residuals are orthogonal to the predictors

```{r, echo=TRUE, warning=FALSE}
sum(residuals*bodyfat[1:length(names(bodyfat))-1])
```

The residuals are orthogonal to the fitted values

```{r, echo=TRUE, warning=FALSE}
sum(residuals*Y_hat)
```

\newpage

# Exercise 6

We use regsubsets to find the best model combinations by adjusted $R^{2}$

```{r, echo=TRUE, warning=FALSE}
options(na.action = "na.fail")
modall <- lm(hwfat ~., data = bodyfat)
combs <- leaps::regsubsets(bodyfat[,1:6],bodyfat[,7])
summary(combs)
```

And their corresponding $R^{2} values:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
summary(combs)$adjr2
```

We can see the best model is the one with the following $R^{2}$

```{r, echo=TRUE, warning=FALSE, message=FALSE}
max(summary(combs$adjr2))
```

Which corresponds to the model which uses the variables **age**, **ht**, **abs** and **triceps**.

We can also do this same calculation using the function dredge (albeit less efficiently):

```{r, echo=TRUE, warning=FALSE, message=FALSE}
combs <- dredge(modall, extra = "R^2")
print("best model")
combs[combs$"R^2" == max(combs$"R^2")]
```

\newpage

# Exercise 7

We know that:

$SST = \sum_{i=1}^{n} (y_{i}-\bar{y})^{2}\\
= \sum_{i=1}^{n} (y_{i} - \hat{y_{i}} + \hat{y_{i}} - \bar{y})^{2}\\
= \sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^{2} + 2 \sum_{i=1}^{n} (y_{i} - \hat{y_{i}}) (\hat{y_{i}} - \bar{y}) + \sum_{i=1}^{n} (\hat{y_{i}} - \bar{y})^{2}\\
= SSE + SSR + 2 \sum_{i=1}^{n} (y_{i}-\hat{y_{i}})(\hat{y_{i}}-\bar{y})$

No we must prove that:

$2 \sum_{i=1}^{n} (y_{i}-\hat{y_{i}})(\hat{y_{i}}-\bar{y}) = 0$

So then we have:

$\sum_{i=1}^{n} (y_{i}-\hat{y_{i}})(\hat{y_{i}}-\bar{y}) = \sum_{i=1}^{n} (y_{i} - \hat{y_{i}}) \hat{y_{i}} - \sum_{i=1}^{n} (y_{i} - \hat{y_{i}}) \bar{y} = 0$

Because we know that:

$\sum_{i=1}^{n} (y_{i} - \hat{y_{i}}) \hat{y_{i}} = 0$ 

Given that the residuals must be orthogonal to the fitted values.

And:

$\sum_{i=1}^{n} (y_{i} - \hat{y_{i}}) \bar{y} = 0$

Because the sum of the observed data is equal to the sum of the fitted values:

$\sum_{i=1}^{n} y_{i} = \sum_{i=1}^{n} \hat{y_{i}}$

\newpage

# Exercise 8

We define a list with all the models excluding, in each one, a single variable.

```{r, echo=TRUE, warning=FALSE}
models <- list()
vars <- c("age","ht","wt","abs","triceps","subscap")
models[[1]] <- update(modall,.~.-age)
models[[2]] <- update(modall,.~.-ht)
models[[3]] <- update(modall,.~.-wt)
models[[4]] <- update(modall,.~.-abs)
models[[5]] <- update(modall,.~.-triceps)
models[[6]] <- update(modall,.~.-subscap)
```

We run ANOVA with both the models without each variable and the main model including all the other variables.

We can see the pvalues for the ANOVA where each specific variable was excluded:

```{r, echo=TRUE, warning=FALSE}
anovas <- list()
pvalues <- c()
amount_of_vars <- length(names(bodyfat))-1
for (i in 1:amount_of_vars) {
    anovas[[i]] <- anova(models[[i]],modall)
    pvalues <- c(pvalues, sum(anovas[[i]][2,"Pr(>F)"]))
}
for (i in 1:length(vars)) {
    print(paste("excluding: ", vars[i], ": ", pvalues[i] , sep=""))
}
```

Then we compare with summary:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
summary(modall)[4]
```

And we can see we get the same pvalues in the summary. Therefore viewing the summary can be a much faster version of performing such testing.

as a result we get that the least meaningful variable (the variable that explains the lowest variance of the model) is the variable ht (height) followed by the variable wt (weight).

\newpage

# Exercise 9

Given that $E[\hat{Y} | X_{h}] = \hat{Y_{h}} \sim N(X_{h} \beta, \sigma^{2} X_{h} (X^{\prime} X) X_{h}^{\prime})$

$\Rightarrow \hat{y_{h}} \pm t_{n - (k+1), \frac{\alpha}{2}} * \hat{\sigma} \sqrt{h_{ii}}$

where $h_{ii}$ is the diagonal of our $H$ matrix.

is our expression for the $(1-\alpha)\%$ confidence interval for $\hat{Y_{h}}$ when $\sigma^{2}$ is unknown.

# Exercise 10

```{r, echo=TRUE, warning=FALSE, message=FALSE}
minmax_scaler <- function(x) {
    return((x-min(x))/(max(x)-min(x)))
}
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
transform <- data.frame(read.table('../data/Transform_V2.txt', header=TRUE))
trm1 <- lm(y ~ sqrt(x2+1), data=transform)
trm2 <- lm(y ~ I(x3^(2)), data=transform)
par(mfrow=c(2,2))
plot(trm1)
```

We can see that the square root transformation is appropriate for the $X_{2}$ variable

\newpage

```{r, echo=TRUE, warning=FALSE, message=FALSE}
par(mfrow=c(2,2))
plot(trm2)
```

We can see that the  $X^{2}$ transformation is appropriate for the $X_{3}$ variable

\newpage

# Exercise 11

```{r, echo=TRUE, warning=FALSE, message=FALSE}
bxcx_transf <- function(x, lambda) {
    if (lambda == 0) {
        return(log(x))
    } else {
        return((x^(lambda)-1)/lambda)
    }
}
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
transform2 <- data.frame(read.table('../data/Transform2_V2.txt', header=TRUE))
trm1 <- lm(y2 ~ x1, data=transform2)
trm2 <- lm(y2 ~ x2, data=transform2)
par(mfrow=c(2,4))
plot(trm1)
plot(trm2)
```

Neither variable has constant variance, therefore we apply a boxcox transformation to both $X_{2}$ and $X_{3}$.

\newpage

```{r, echo=TRUE, warning=FALSE, message=FALSE}
boxcox(trm2)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
boxcox(trm1)
```

\newpage

As $-1$ is in the confidence interval for both models, we will apply a boxcox transformation using $\lambda = -1$.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
transform2 <- data.frame(read.table('../data/Transform2_V2.txt', header=TRUE))
trm1 <- lm(bxcx_transf(y2,-1) ~ x1, data=transform2)
trm2 <- lm(bxcx_transf(y2,-1) ~ x2, data=transform2)
par(mfrow=c(2,4))
plot(trm1)
plot(trm2)
```

Here we see the situation has improved significantly and we now have constant variance.

\newpage

# Exercise 12



\newpage

# Exercise 13

Using R's function to calculate VIF for our model:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vif(modall)
```

We can see that the VIF score for **triceps**, **abs**, **wt** and **subscap** are all above 10 and therefore very high.

Programming my own VIF function:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vif_mc <- function(X) {
    for (i in 1:(length(names(X)))) {
        preds <- 1:length(names(X))
        preds <- preds[preds != i]
        model <- lm(X[,i] ~ X[,preds])
        print(model)
    }
}
```
