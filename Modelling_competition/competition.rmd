---
title: 'title'
author: 'THE REGRESSORS'
date: 'January 14th, 2021'
output: 'pdf_document'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = '#>',
fig.path = './figures/'
)
knitr::knit_engines$set(julia = JuliaCall::eng_juliacall)
options(JULIA_HOME = '/home/dreth/julia/bin')
```

Importing libraries:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(stringr)
library(gridExtra)
library(outliers)
library(PerformanceAnalytics)
library(foreach)
library(MASS)
library(e1071) 
library(VGAM)
library(caret)
library(klaR)
library(arm)
library(caTools)
library(stepPlr)
library(LiblineaR)
library(caret)
library(Epi)
library(ROSE)
library(ResourceSelection)
```

Importing and manipulating the data:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
credit <- read.csv('./data/credit.csv')
names(credit) <- tolower(names(credit))
```

Basic variable selection:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vars <- c("obs.","chk_acct","duration","history",
          "new_car","used_car","furniture","radio.tv",
          "education","retraining","amount","sav_acct",
          "employment","install_rate","male_div","male_single",
          "male_mar_or_wid","co.applicant","guarantor","present_resident",
          "real_estate","prop_unkn_none","age","other_install",
          "rent","own_res","num_credits","job",
          "num_dependents","telephone","foreign","response")
          
vars_to_remove <- c("own_res", "obs.", "real_estate")

credit <- credit %>% dplyr::select(setdiff(vars,vars_to_remove))
credit$response <- as.factor(credit$response)
names(credit)
```

Given that *amount* is NOT normally distributed, we take the log of it: tits

```{r, echo=TRUE, warning=FALSE, message=FALSE}
credit$amount <- log(credit$amount + 1)
credit$age <- log(credit$age + 1)
credit$duration <- log(credit$duration + 1)
```

TTS:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
set.seed(12)
spl = caret::createDataPartition(credit$response, p = 0.8, list = FALSE)
Train = credit[spl,]
Test = credit[-spl,]
Train$response <- as.factor(Train$response)
Test$response <- as.factor(Test$response)
```

We define a function to obtain the formula of all models with and without a chosen amount of interactions (2-way, 3-way, etc):

```{r, echo=TRUE, warning=FALSE, message=FALSE}
model_formula <- function(data, combs, target, with_int=TRUE, all=FALSE) {
    formulas <- c()
    cols <- names(data)[1:(length(names(data))-1)]
    combinations <- combinat::combn(cols, combs)
    for (i in 1:length(combinations[1,])) {
        if (with_int == TRUE) {
            if (all == TRUE) {
                form_pst <- paste(combinations[,i], collapse="*")
                form <- stringr::str_interp("${target}~${form_pst}")
                formulas <- c(formulas, form)
            } else {
                form_pst <- paste(combinations[,i], collapse="+")
                form <- stringr::str_interp("${target}~(${form_pst})^${all}")
                formulas <- c(formulas, form)
            }
        } else {
            form_pst <- paste(combinations[,i], collapse="+")
            form <- stringr::str_interp("${target}~${form_pst}")
            formulas <- c(formulas, form)
        }
    }
    return(formulas)
}
```

Modelling function:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
modelling <- function(data, formulas) {
    models <- list()
    for (i in 1:length(formulas)) {
        models[[i]] <- glm(formula=formulas[i], family=binomial, data=data)
    }
    return(models)
}
```

LRT for models with and without interactions:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
test <- function(formulas_with, formulas_without, models_with_int, models_without_int) {
    p_vals <- c()
    aic_with <- c()
    aic_without <- c()
    for (i in 1:length(formulas_with)) {
        p_vals <- c(p_vals, anova(models_with_int[[i]], models_without_int[[i]], test="Chisq")$"Pr(>Chi)"[2])
        aic_with <- c(aic_with, AIC(models_with_int[[i]]))
        aic_without <- c(aic_without, AIC(models_without_int[[i]]))
    }
    return(data.frame(formulas_with=formulas_with, formulas_without=formulas_without, pvals=p_vals, aic_with=aic_with, aic_without=aic_without))
}
```

Scoring function:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
scoring <- function(data, testing, models, formulas) {
    accuracy <- c()
    roc_cutoff <- c()
    roc_auc <- c()
    roc_sensitivity <- c()
    roc_specificity <- c()
    # hoslem <- c()
    for (i in 1:length(models)) {
        # ROC curve
        roc1 <- Epi::ROC(form=formula(models[[i]]), data=data, plot="ROC", lw=3, cex=1.5)
        cutoff <- which.max(rowSums(roc1$res[, c("sens", "spec")]))

        # ROC params
        roc_cutoff <- c(roc_cutoff, roc1$res$lr.eta[cutoff])
        roc_auc <- c(roc_auc, roc1$AUC)
        roc_sensitivity <- c(roc_sensitivity, roc1$res$sens[cutoff])
        roc_specificity <- c(roc_specificity, roc1$res$spec[cutoff])

        # prediction using BEST cutoff
        prediction <- predict(models[[i]], newdata=testing, type="response")
        prediction <- ifelse(prediction > roc1$res$lr.eta[cutoff], 1, 0)
        pred <- as.factor(prediction)

        # target score 
        real_vals <- as.factor(testing$response)

        # hosmer lemeshow goodness of fit test
        # hltest <- hoslem.test(real_vals, prediction)$p.value
        # hoslem <- c(hoslem, hltest)

        # confusion matrix score
        accuracy <- c(accuracy,caret::confusionMatrix(pred, real_vals)$overall[1])
    }
    return(data.frame(formula=formulas,
                      accuracy=accuracy, 
                      cutoff=roc_cutoff, 
                      roc_auc=roc_auc, 
                      sensitivity=roc_sensitivity, 
                      specificity=roc_specificity))
}
```

### We run a model with *all the variables*

```{r, echo=TRUE, warning=FALSE, message=FALSE}
mod <- glm(formula=response~., family=binomial, data=Train)
roc1 <- Epi::ROC(form=formula(mod), data=Train, plot="ROC", lw=3, cex=1.5)
cutoff <- which.max(rowSums(roc1$res[, c("sens", "spec")]))
prediction <- predict(mod, newdata=Test, type="response")
prediction <- ifelse(prediction > roc1$res$lr.eta[cutoff], 1, 0)
pred <- as.factor(prediction)
caret::confusionMatrix(pred, Test$response)
```

Using all our variables we reach an accuracy of 0.785, which so far is decent, but this can improve. 

### Testing 1-variable models

1-variable models:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
cols <- names(credit)[1:(length(names(credit))-1)]
vars <- c()
acc <- c()
for (i in 1:length(cols)) {
    # formula and model
    form <- stringr::str_interp("response~${cols[i]}")
    mod <- glm(formula=form, family=binomial, data=Train)
    form <- formula(mod)

    # ROC curve and cutoff
    roc1 <- Epi::ROC(form=form, data=Train, plot="ROC", lw=3, cex=1.5)
    cutoff <- roc1$res$lr.eta[2]

    # prediction
    pred <- predict(mod, newdata=Test, type="response")
    pred <- ifelse(pred > cutoff, 1, 0)
    pred <- as.factor(pred)

    # confusion matrix and accuracy
    Accuracy <- caret::confusionMatrix(pred, Test$response)$overall[1]
    
    # adding variables to prediction
    vars <- c(vars, cols[i])
    acc <- c(acc, Accuracy)
}
df <- data.frame(vars=vars, accuracy=acc)
df <- df[order(-df$accuracy),][1:10,]
knitr::kable(
    df,
    booktabs=TRUE,
    longtable=TRUE,
    caption="best models"
)
```

We can see that (individually), the best variables that we obtain are *chk_acct*, *duration*, *amount*, *age*, *history*, *education*, *co.applicant*, *retraining*, *other_install* and *prop_unkn_none*.

### Testing models with and without variables to check significance

Here we perform LRT to check the significance of individual variables (lower is better):

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sig_scores <- c()
vars <- c()
aics <- c()
cols <- names(credit)[names(credit) != "response"]
mod1 <- glm(formula=response~., family=binomial, data=Train)
for (i in 1:length(cols)) {
    # variables
    vars_selected <- setdiff(names(credit), c(names(credit)[i],"response"))
    # formulas
    formula2 <- stringr::str_interp("response~${paste(vars_selected, collapse='+')}")
    vars <- c(vars, cols[i])
    # model and ROC
    mod2 <- glm(formula=formula2, family=binomial, data=Train)
    # pval
    sig_scores <- c(sig_scores, anova(mod1, mod2, test="Chisq")$"Pr(>Chi)"[2])
    # AIC
    aics <- c(aics, AIC(mod2))
}
df <- data.frame(vars=vars,scores=sig_scores,aic=aics)
knitr::kable(
    df[order(-df$scores),],
    booktabs=TRUE,
    longtable=TRUE,
    caption="best models"
)
```

This allows us to remove *furniture* and *male_mar_or_wid* which seem to not be signficant.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
vars <- c("obs.","chk_acct","duration","history",
          "new_car","used_car","furniture","radio.tv",
          "education","retraining","amount","sav_acct",
          "employment","install_rate","male_div","male_single",
          "male_mar_or_wid","co.applicant","guarantor","present_resident",
          "real_estate","prop_unkn_none","age","other_install",
          "rent","own_res","num_credits","job",
          "num_dependents","telephone","foreign","response")
          
vars_to_remove <- c("own_res", "obs.", "real_estate", "furniture", "male_mar_or_wid")

credit <- credit %>% dplyr::select(setdiff(vars,vars_to_remove))
credit$response <- as.factor(credit$response)
names(credit)
```

### Testing 2-variable models with and without interactions

We create models with all the combinations of 2 variables and then we perform LRT for models with and without interactions. Then we select models with an LRT p-value under 0.005, in order to keep the most important interactions.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
formulas_with <- model_formula(credit, 2, "response", with_int=TRUE, all=2)
formulas_without <- model_formula(credit, 2, "response", with_int=FALSE)
models_with <- modelling(Train, formulas_with)
models_without <- modelling(Train, formulas_without)
```

We run the tests:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
two_var_combs <- test(formulas_with, formulas_without, models_with, models_without)
```

We remove NAs, given that these interactions' product is 0 for all values, therefore, the LRT returns a p-value of 1 (meaning there's no difference between the models).

```{r, echo=TRUE, warning=FALSE, message=FALSE}
two_var_combs <- na.omit(two_var_combs[order(-two_var_combs$pvals),])
best_2_vars <- two_var_combs[two_var_combs$pvals < 0.005,]
```

We present the table showing the model formulas and the p-values:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
knitr::kable(
    best_2_vars,
    booktabs=TRUE,
    longtable=TRUE,
    caption="best models"
)
```

### Testing a model using the best 2-way interactions

We also use all the variables as we did previously, however, we include the interactions selected:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_numbers <- as.numeric(rownames(best_2_vars))
all_vars <- list()
for (i in 1:length(model_numbers)) {
    all_vars[[i]] <- all.vars(formula(models_with[[model_numbers[i]]])[-2])
    all_vars[[i]] <- c(all_vars[[i]], paste(all_vars[[i]],collapse=":"))
}
all_vars

two_vars <- c()
for (i in 1:length(all_vars)) {
    two_vars <- c(two_vars, all_vars[[i]][1],all_vars[[i]][3])
}
two_vars <- unique(two_vars)
two_vars
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vars_selected <- unique(c(two_vars, names(credit)))
mod <- glm(formula=str_interp("response~${paste(vars_selected, collapse='+')}"), family=binomial, data=Train)
roc1 <- Epi::ROC(form=formula(mod), data=Train, plot="ROC", lw=3, cex=1.5)
cutoff <- which.max(rowSums(roc1$res[, c("sens", "spec")]))
prediction <- predict(mod, newdata=Test, type="response")
prediction <- ifelse(prediction > roc1$res$lr.eta[cutoff], 1, 0)
pred <- as.factor(prediction)
caret::confusionMatrix(pred, Test$response)
```

The model didn't improve much given that we have essentially the same accuracy, but we have higher sensitivity, therefore we keep testing.

### Testing 3-variable models with and without interactions

We create models with all the combinations of 3 variables and then we perform LRT for models with and without interactions. Then we select models with an LRT p-value under 0.0005, in order to keep the most important interactions.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
formulas_with <- model_formula(credit, 3, "response", with_int=TRUE, all=3)
formulas_without <- model_formula(credit, 3, "response", with_int=FALSE)
models_with <- modelling(Train, formulas_with)
models_without <- modelling(Train, formulas_without)
```

We run the tests:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
three_var_combs <- test(formulas_with, formulas_without, models_with, models_without)
```

We remove NAs, given that these interactions' product is 0 for all values, therefore, the LRT returns a p-value of 1 (meaning there's no difference between the models).

```{r, echo=TRUE, warning=FALSE, message=FALSE}
three_var_combs <- na.omit(three_var_combs[order(-three_var_combs$pvals),])
best_3_vars <- three_var_combs[three_var_combs$pvals < 0.0005,]
```

We present the table showing the model formulas and the p-values:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(
    best_3_vars,
    booktabs=TRUE,
    longtable=TRUE,
    caption="best models"
)
```


### Testing a model using the best 3-way and 2-way interactions

We also use all the variables as we did previously, however, we include the interactions selected:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
model_numbers <- as.numeric(rownames(best_3_vars))
all_vars <- list()
for (i in 1:length(model_numbers)) {
    all_vars[[i]] <- all.vars(formula(models_with[[model_numbers[i]]])[-2])
    all_vars[[i]] <- c(all_vars[[i]], paste(all_vars[[i]],collapse=":"))
}
all_vars

three_vars <- c()
for (i in 1:length(all_vars)) {
    three_vars <- c(three_vars, all_vars[[i]][1],all_vars[[i]][4])
}
three_vars <- unique(three_vars)
three_vars
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vars_selected <- unique(c(three_vars, names(credit), two_vars))
mod <- glm(formula=str_interp("response~${paste(vars_selected, collapse='+')}"), family=binomial, data=Train)
roc1 <- Epi::ROC(form=formula(mod), data=Train, plot="ROC", lw=3, cex=1.5)
cutoff <- which.max(rowSums(roc1$res[, c("sens", "spec")]))
prediction <- predict(mod, newdata=Test, type="response")
prediction <- ifelse(prediction > roc1$res$lr.eta[cutoff], 1, 0)
pred <- as.factor(prediction)
caret::confusionMatrix(pred, Test$response)
```

We remove the variables *radio.tv:retraining:age* and *used_car:amount:education* given that they have NA coefficients:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
vars_selected <- unique(c(three_vars, names(credit), two_vars))
vars_selected <- vars_selected[vars_selected != "used_car:education:amount" & vars_selected != "radio.tv:retraining:age"]
mod <- glm(formula=str_interp("response~${paste(vars_selected, collapse='+')}"), family=binomial, data=Train)
roc1 <- Epi::ROC(form=formula(mod), data=Train, plot="ROC", lw=3, cex=1.5)
cutoff <- which.max(rowSums(roc1$res[, c("sens", "spec")]))
prediction <- predict(mod, newdata=Test, type="response")
prediction <- ifelse(prediction > roc1$res$lr.eta[cutoff], 1, 0)
pred <- as.factor(prediction)
caret::confusionMatrix(pred, Test$response)
```

